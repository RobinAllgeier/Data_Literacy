\section{Data and Methods}\label{sec:methods}

%In this section, describe \emph{what you did}. Roughly speaking, explain what data you worked with, how or from where it was collected, it's structure and size. Explain your analysis, and any specific choices you made in it. Depending on the nature of your project, you may focus more or less on certain aspects. If you collected data yourself, explain the collection process in detail. If you downloaded data from the net, show an exploratory analysis that builds intuition for the data, and shows that you know the data well. If you are doing a custom analysis, explain how it works and why it is the right choice. If you are using a standard tool, it may still help to briefly outline it. Cite relevant works. You can use the \verb|\citep| and \verb|\citet| commands for this purpose \citep{mackay2003information}.

% This is the template for a figure from the original ICML submission pack. In lecture 10 we will discuss plotting in detail.
% Refer to this lecture on how to include figures in this text.
% 
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

\subsection{Data Collection and Description}
This study investigates the borrowing behavior of users at the Tübingen City Library, as well as the frequency of late returns. The dataset was provided directly by the Tübingen City Library and covers borrowing records from 2019 to 2025. It encompasses over 2.4 million individual loan transactions and includes the following key variables: borrowing and return timestamps, 21 different media types, user categories, number of extensions, anonymized user identification numbers, as well as late return flags. Each record documents a complete transaction from initial borrowing to return.
Domain knowledge was gathered through a personal interview with a staff member of the Tübingen City Library, providing contextual insights into the data collection processes, library operations, and data quality practices. Additional information was obtained through ongoing email communication with the library staff, which helped to clarify data inconsistencies and provide domain expertise for informed analysis decisions.

\subsection{Data Quality Assessment: Sanity Checks}
Prior to conducting any analysis, a comprehensive set of data quality checks was performed to validate internal consistency and identify potential data quality issues. These sanity checks were designed to detect anomalies without filtering or removing data at first, and gather information, regarding cleaning the data to ensure robust analysis afterwards. The following specific checks were implemented:

\subsubsection{Missing Values}
First of all, the overall of missing values in the dataset was assessed. The proportion of missing values in each column was calculated to identify any fields with significant gaps that could impact the analysis. Depending on the extent and nature of the missing valus, they were excluded from certain analyses.
Return timestamps are missing in roughly two percent of rows, which is expected for currently borrowed and unreturned or lost items and therefore retained. The same rate applies to the derived duration column because it depends on return timestamps. A missing rate of about 6.7\% was found in the user ID colum, which will be handled accordingly in the analysis. All other colums, which are relevant for our analyses, have a missing rate below 0.1\% and are therefore considered complete.

\subsubsection{Timestamp and Duration Consistency}
The integrity of temporal data was verified by identifying logical inconsistencies, such as instances where return dates preceded borrowing dates. These checks are critical since all downstream analyses depend on accurate temporal information. Further checks on the duration value revealed that it is calculated correctly from the timestamps and does not contain any inconsistencies.

\subsubsection{Late Return Consistency}
The consistency between late return flags and the reported number of days late was validated. No Entries were identified where the late flag indicated ``not late'' but the days-late counter was positive, and conversely, records marked as late but with zero or missing days-late values. Additionally, implausible values in the extensions column (negative extensions or more than six extensions) were checked. There are a few cases where the number of extensions is higher than six, which should not be possible according to the library's policies. But because of the low number of affected rows (0.003\%), these where not further investigated.

\subsubsection{Duplicate Analysis}
Various forms of duplicates were examined. There are no exact duplicates in the dataset. However, some entries show the the exact same borrowing timestamp. These can be explained by the fact, that users can borrow at multiple stations at the same time. Also it is possible for user to borrow multiple items in one transaction, resulting in identical borrowing timestamps for different items, but all these occurrences limit to the same user ID and to a maximum of seven items/entries per transaction. Only one case was found where a different user borrowed and returned at the exact same times, which is considered a rare but possible event and therefore retained.

\subsection{Data cleaning}
Based on the result of the data quality assessment and the analysis approach defined above, the data was cleaned, based on four main rules:
\begin{itemize}
    \item Entries with missing retuern timestamps were retained, as they represent currently borrowed or lost items.
    \item Entries with a borrowing time longer than the 196 days were removed, because they extend the maximum borrowing period of 28 days and six extentions, which are considered implausible.
    \item Borrowings taken by users with the user type MDA and SYS were removed, because these represent borrowings from the library`s staff members and system entries, which do not reflect typical user behavior.
    \item For some analyses, all media types exept for books were removed, because they have different borrowing duration limits and would distort the results.
\end{itemize}
These cleaning steps ensured that the dataset used for analysis was both reliable and relevant, thereby enhancing the validity of the findings.


\subsection{Analysis Methods}

To move from item-level borrowing records to user-level behavioral analysis, individual borrowings were aggregated into user-specific sessions. 
Each record corresponds to a single item, while users often borrow multiple items during one library visit. 
A session is therefore defined as all borrowings by the same user on the same calendar day. 
Sessions were ordered chronologically per user and assigned a session index, which serves as a proxy for increasing experience. 
Session-level indicators were derived by aggregation. A session was marked as late if any item was returned after the due date and as extended if any item received a loan extension. This session-based representation forms the common basis for all subsequent analyses.

Based on this representation, temporal usage patterns and visit regularity were quantified using borrowing timestamps. 
Borrowing events were aggregated by weekday and hour, and the number of active users was computed in hourly bins to summarize population-level usage. 
Individual visit regularity was measured by defining a typical borrowing time for each user as the modal weekday and hour across their borrowing history. 
Each event was labeled according to whether it matched this typical time. Aggregating this indicator across users yields a population-level measure of regularity.

Behavioral adaptation over time was analyzed using the session index as a measure of user experience. 
For each experience level $k$, we computed the proportion of late sessions and sessions with extensions across all users who reached at least $k$ sessions. 
This ensures that each user contributes at most one observation per level. Let $L_{u,k}$ denote whether the $k$-th session of user $u$ contains at least one late item. 
The late-return learning curve is
\[
\hat{p}_{L}(k) = \frac{1}{|U_k|} \sum_{u \in U_k} L_{u,k},
\]
where $U_k$ denotes the set of users with at least $k$ sessions. Extensions were computed analogously.

Uncertainty was estimated using user-level bootstrap resampling. 
Because observations are dependent within users and the sampling distribution of $\hat{p}_L(k)$ is unknown, parametric methods are inappropriate. 
Users were therefore resampled with replacement, session sequences were reconstructed, and learning curves recomputed for each bootstrap sample. 
Confidence intervals were obtained from the resulting empirical distributions \cite{Davison_Hinkley_1997}.
